#!/usr/bin/env python3
"""
Mycelium v6: Train Qwen-0.5B Segmenter with IO Tagging

2-class token classification:
- O (0): Outside any span
- I (1): Inside a span

Contiguous I runs define span boundaries. No B tag needed.
This approach achieved 86.5% F1.
"""

import json
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
from datasets import Dataset
from transformers import (
    AutoModel,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForTokenClassification,
    PreTrainedModel,
)
from transformers.modeling_outputs import TokenClassifierOutput
import random
import argparse


class Qwen2ForTokenClassification(PreTrainedModel):
    """Qwen2 with a token classification head."""

    def __init__(self, config, num_labels=2):
        super().__init__(config)
        self.num_labels = num_labels
        self.qwen = AutoModel.from_pretrained(
            config._name_or_path,
            config=config,
            trust_remote_code=True,
        )
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(config.hidden_size, num_labels)

        # Initialize classifier weights
        self.classifier.weight.data.normal_(mean=0.0, std=0.02)
        self.classifier.bias.data.zero_()

    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):
        outputs = self.qwen(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **kwargs
        )
        sequence_output = outputs.last_hidden_state
        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        return TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


# IO Labels (2-class)
LABELS = ["O", "I"]
label2id = {l: i for i, l in enumerate(LABELS)}
id2label = {i: l for i, l in enumerate(LABELS)}

MODEL_NAME = "Qwen/Qwen2-0.5B"


def load_data(data_path: str):
    """Load IO-labeled segmentation data."""
    with open(data_path) as f:
        data = json.load(f)

    print(f"Loaded {len(data)} examples")

    # Filter out examples with no I labels (no spans)
    data = [d for d in data if 1 in d["labels"]]
    print(f"After filtering 0-span examples: {len(data)}")

    # Count spans (Oâ†’I transitions)
    total_spans = 0
    for d in data:
        labels = d["labels"]
        for i in range(len(labels)):
            if labels[i] == 1 and (i == 0 or labels[i-1] == 0):
                total_spans += 1
    print(f"Total spans: {total_spans:,} ({total_spans/len(data):.1f} per sample)")

    return data


def align_labels(orig_labels: list, orig_len: int, new_len: int) -> list:
    """
    Align labels from original tokenization to new tokenization.

    Uses proportional mapping: for each new token at position i,
    assign the label from position round(i * orig_len / new_len).

    This works well when tokenizations are similar (same tokenizer family).
    """
    if orig_len == new_len:
        return orig_labels[:new_len]

    aligned = []
    for i in range(new_len):
        # Map new position to original position
        orig_pos = int(round(i * (orig_len - 1) / (new_len - 1))) if new_len > 1 else 0
        orig_pos = min(orig_pos, len(orig_labels) - 1)
        aligned.append(orig_labels[orig_pos])

    return aligned


def prepare_dataset(data: list, tokenizer, max_length=256):
    """Prepare dataset for training."""
    processed = []
    skipped = 0

    for example in data:
        # Handle both formats: input_ids or text
        if "input_ids" in example:
            input_ids = example["input_ids"][:max_length]
            labels = example["labels"][:max_length]
        elif "text" in example:
            # Tokenize text with Qwen tokenizer
            encoding = tokenizer(
                example["text"],
                truncation=True,
                max_length=max_length,
                add_special_tokens=True,
            )
            input_ids = encoding["input_ids"]

            # Align labels from original tokenization to Qwen tokenization
            orig_labels = example["labels"]
            labels = align_labels(orig_labels, len(orig_labels), len(input_ids))
        else:
            skipped += 1
            continue

        # Pad to max_length
        pad_len = max_length - len(input_ids)
        if pad_len > 0:
            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len
            labels = labels + [-100] * pad_len  # -100 ignores loss

        processed.append({
            "input_ids": input_ids,
            "attention_mask": [1] * (max_length - pad_len) + [0] * pad_len,
            "labels": labels,
        })

    if skipped > 0:
        print(f"  Skipped {skipped} examples (missing input_ids or text)")

    return Dataset.from_list(processed)


def extract_spans_io(labels: list) -> list:
    """
    Extract (start, end) spans from IO label sequence.

    Contiguous runs of I (1) tokens form spans.
    """
    spans = []
    current_start = None

    for i, label in enumerate(labels):
        if label == -100:
            continue

        if label == 1:  # I (Inside)
            if current_start is None:
                current_start = i
        else:  # O (Outside) or other
            if current_start is not None:
                spans.append((current_start, i))
                current_start = None

    # Handle span at end
    if current_start is not None:
        spans.append((current_start, len(labels)))

    return spans


def compute_span_metrics(pred_labels, true_labels):
    """Compute span-level precision, recall, F1 for IO tagging."""
    tp = 0
    total_pred = 0
    total_true = 0

    for preds, trues in zip(pred_labels, true_labels):
        pred_spans = extract_spans_io(preds)
        true_spans = extract_spans_io([t for t in trues if t != -100])

        total_pred += len(pred_spans)
        total_true += len(true_spans)

        # Match spans with >50% overlap
        for pred in pred_spans:
            for true in true_spans:
                overlap_start = max(pred[0], true[0])
                overlap_end = min(pred[1], true[1])
                overlap = max(0, overlap_end - overlap_start)

                pred_len = pred[1] - pred[0]
                true_len = true[1] - true[0]

                if overlap > 0.5 * min(pred_len, true_len):
                    tp += 1
                    break

    precision = tp / total_pred if total_pred > 0 else 0
    recall = tp / total_true if total_true > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return {
        "span_precision": precision,
        "span_recall": recall,
        "span_f1": f1,
        "total_pred_spans": total_pred,
        "total_true_spans": total_true,
    }


def compute_metrics(eval_pred):
    """Compute evaluation metrics."""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=2)

    true_flat = []
    pred_flat = []

    for pred_seq, label_seq in zip(predictions, labels):
        for p, l in zip(pred_seq, label_seq):
            if l != -100:
                true_flat.append(l)
                pred_flat.append(p)

    correct = sum(1 for p, t in zip(pred_flat, true_flat) if p == t)
    token_accuracy = correct / len(true_flat) if true_flat else 0

    # Per-class accuracy
    i_correct = sum(1 for p, t in zip(pred_flat, true_flat) if p == t and t == 1)
    i_total = sum(1 for t in true_flat if t == 1)
    o_correct = sum(1 for p, t in zip(pred_flat, true_flat) if p == t and t == 0)
    o_total = sum(1 for t in true_flat if t == 0)

    i_accuracy = i_correct / i_total if i_total > 0 else 0
    o_accuracy = o_correct / o_total if o_total > 0 else 0

    span_metrics = compute_span_metrics(predictions.tolist(), labels.tolist())

    return {
        "token_accuracy": token_accuracy,
        "i_accuracy": i_accuracy,
        "o_accuracy": o_accuracy,
        **span_metrics,
    }


def main():
    parser = argparse.ArgumentParser(description="Train C1 Segmenter")
    parser.add_argument("--data-path", default="data/c1_iaf/c1_training_data_merged.json",
                        help="Path to training data")
    parser.add_argument("--output-dir", default=None,
                        help="Output directory for model")
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--batch-size", type=int, default=16)
    parser.add_argument("--learning-rate", type=float, default=3e-5)
    parser.add_argument("--max-length", type=int, default=256)
    args = parser.parse_args()

    print("=" * 60)
    print("MYCELIUM V6: C1 SEGMENTER TRAINING (IO TAGGING)")
    print("=" * 60)

    # Load tokenizer
    print("\nLoading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load data
    print(f"\nLoading training data from {args.data_path}...")
    data = load_data(args.data_path)

    # Split train/eval
    random.seed(42)
    random.shuffle(data)
    split_idx = int(len(data) * 0.9)
    train_data = data[:split_idx]
    eval_data = data[split_idx:]

    print(f"Train: {len(train_data)}, Eval: {len(eval_data)}")

    # Prepare datasets
    print("\nPreparing datasets...")
    train_dataset = prepare_dataset(train_data, tokenizer, args.max_length)
    eval_dataset = prepare_dataset(eval_data, tokenizer, args.max_length)

    # Load model
    print("\nLoading model...")
    from transformers import AutoConfig
    config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)
    model = Qwen2ForTokenClassification(config, num_labels=len(LABELS))

    # Set label mappings on config
    model.config.id2label = id2label
    model.config.label2id = label2id

    # Convert to bfloat16
    model = model.to(torch.bfloat16)

    print(f"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M")

    # Training arguments
    if args.output_dir:
        output_dir = args.output_dir
    elif Path("/opt/dlami/nvme").exists():
        output_dir = "/opt/dlami/nvme/models/c1_segmenter_iaf"
    else:
        output_dir = "models/c1_segmenter_iaf"

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=2,
        learning_rate=args.learning_rate,
        weight_decay=0.01,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="span_f1",
        greater_is_better=True,
        logging_steps=50,
        bf16=True,
        report_to="none",
        save_total_limit=2,
        warmup_ratio=0.1,
    )

    # Data collator
    data_collator = DataCollatorForTokenClassification(
        tokenizer=tokenizer,
        padding=True,
        pad_to_multiple_of=8,
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # Train
    print("\nStarting training...")
    print("=" * 60)
    trainer.train()

    # Final evaluation
    print("\n" + "=" * 60)
    print("FINAL EVALUATION")
    print("=" * 60)

    results = trainer.evaluate()
    for k, v in results.items():
        print(f"  {k}: {v:.4f}" if isinstance(v, float) else f"  {k}: {v}")

    # Save model
    final_path = f"{output_dir}/final"
    print(f"\nSaving model to {final_path}...")
    model.save_pretrained(final_path)
    tokenizer.save_pretrained(final_path)

    # Quick inference test
    print("\n" + "=" * 60)
    print("INFERENCE TEST")
    print("=" * 60)

    test_texts = [
        "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether?",
        "Find $2^{-1} \\pmod{185}$, as a residue modulo 185.",
    ]

    model.eval()
    for test_text in test_texts:
        inputs = tokenizer(test_text, return_tensors="pt", truncation=True, max_length=256)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=-1)[0]

        tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
        labels_pred = [id2label[p.item()] for p in predictions]

        print(f"\nInput: {test_text[:80]}...")
        print("Spans found:")

        # Extract and print spans
        spans = []
        current_start = None
        for i, (tok, lab) in enumerate(zip(tokens, labels_pred)):
            if lab == "I":
                if current_start is None:
                    current_start = i
            else:
                if current_start is not None:
                    span_tokens = tokens[current_start:i]
                    span_text = tokenizer.convert_tokens_to_string(span_tokens)
                    spans.append(span_text)
                    current_start = None

        if current_start is not None:
            span_tokens = tokens[current_start:]
            span_text = tokenizer.convert_tokens_to_string(span_tokens)
            spans.append(span_text)

        for i, span in enumerate(spans):
            print(f"  [{i+1}] {span.strip()}")

    print("\n" + "=" * 60)
    print("TRAINING COMPLETE")
    print("=" * 60)


if __name__ == "__main__":
    main()
